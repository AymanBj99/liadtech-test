# encoder.py
from transformers import AutoTokenizer, AutoModel
import torch
import torch.nn as nn

class TextEncoder(nn.Module):
    def __init__(self, model_name="bert-base-uncased", device=None):
        super(TextEncoder, self).__init__()
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ðŸ”¹ Chargement du modÃ¨le {model_name} sur {self.device}...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name).to(self.device)

    def encode(self, texts, max_length=128, batch_size=8):
        """
        Encode une liste de textes en embeddings BERT
        Retourne un tenseur (nb_textes, 768)
        """
        all_embeddings = []

        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i+batch_size]

            # Tokenisation
            inputs = self.tokenizer(
                batch_texts,
                padding=True,
                truncation=True,
                max_length=max_length,
                return_tensors="pt"
            ).to(self.device)

            # Passage dans BERT
            with torch.no_grad():
                outputs = self.model(**inputs)
                # On prend les embeddings [CLS] comme reprÃ©sentation globale
                embeddings = outputs.last_hidden_state[:, 0, :]  # shape: (batch_size, 768)

            all_embeddings.append(embeddings.cpu())

        return torch.cat(all_embeddings, dim=0)



if __name__ == "__main__":
    from data_loader import load_data

    docs_dict, train_pairs, val_pairs = load_data()
    doc_texts = list(docs_dict.values())

    encoder = TextEncoder()
    doc_embeddings = encoder.encode(doc_texts)

    queries = [p["query"] for p in train_pairs]
    query_embeddings = encoder.encode(queries)
    print("Taille des embeddings (queries) :", query_embeddings.shape)

    print("âœ… Encodage terminÃ© !")
    print("Taille des embeddings :", doc_embeddings.shape) 
